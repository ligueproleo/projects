{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHtiCRz4fNIohzE57eL54r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ligueproleo/Foundation_Trilogy/blob/main/I2A2_Desafio4_word_embedding_Leo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio 4 - I2A2 - Word Embedding\n",
        "## Curso Redes Generativas - 2023 - Leonardo Alves Pereira"
      ],
      "metadata": {
        "id": "xv1YBwgMEoUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Obter o Conteúdo na Web\n",
        "## Livro: Trilogia Fundação de Isaac Asimov\n",
        "\n",
        "#### Esse bloco de comandos é importante para obter o conteúdo de um arquivo de texto da web (neste caso, um livro) e utilizá-lo para as tarefas subsequentes, como processamento de linguagem natural, treinamento de modelos de word embedding, ou análise de texto."
      ],
      "metadata": {
        "id": "2RFkM_d3YsxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL do arquivo raw no GitHub\n",
        "url = \"https://raw.githubusercontent.com/ligueproleo/Foundation_Trilogy/main/Isaac_Asimov_Trilogia_Fundacao_UTF8.txt\"\n",
        "\n",
        "# Requisição para obter o conteúdo do arquivo\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verifica se a requisição foi bem-sucedida (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # O conteúdo do arquivo está em response.text\n",
        "    conteudo_do_arquivo = response.text\n",
        "\n",
        "    # Agora você pode usar 'conteudo_do_arquivo' conforme necessário\n",
        "    # Por exemplo, imprimir as primeiras 1000 caracteres:\n",
        "    print(conteudo_do_arquivo[:1000])\n",
        "else:\n",
        "    print(f\"Erro ao acessar o arquivo. Código de status: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSD1nGomEwpE",
        "outputId": "fbc95660-1bf8-426b-c4f1-a77e03696dc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISAAC ASIMOV\r\n",
            "\r\n",
            "       \r\n",
            "\r\n",
            "\r\n",
            " \r\n",
            " \r\n",
            " \r\n",
            "Fundação\r\n",
            "Fundação e Império\r\n",
            "Segunda Fundação\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Trilogia \r\n",
            "FUNDAÇÃO \r\n",
            "FUNDAÇÃO E IMPÉRIO \r\n",
            "SEGUNDA FUNDAÇÃO\r\n",
            "\r\n",
            "\r\n",
            "PREFÁCIO\r\n",
            "\r\n",
            "      Uma obra de ficção implica geralmente em um pouco mais que saber \r\n",
            "escrever bem. O conhecimento científico é desejável, mas, acima de tudo, o que \r\n",
            "importa é imaginação.\r\n",
            "      Isaac Asimov talvez seja o maior polígrafo de nossa época, pelo menos e \r\n",
            "como registra a 115ª Edição da Enciclopédia Galáctica, e seus trabalhos se \r\n",
            "estendem desde obras de vulgarização cientifica, como O Corpo Humano e O \r\n",
            "Cérebro Humano em que um estilo leve e vivo estabelece um estudo completo do \r\n",
            "animal \"Homo Sapiens\", até os altos vôos de imaginação em que todo um novo \r\n",
            "Cosmos é construído, na trilogia que é atualmente um clássico: Fundação.\r\n",
            "      Os críticos literários normalmente afetam um total desconhecimento das \r\n",
            "obras de ficção cientifica, pretendendo com isso afetar uma ausência de conteúdo \r\n",
            "artístico neste gênero\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Os comandos desse bloco realizam as seguintes tarefas\n",
        "\n",
        "> url = \"https://raw.githubusercontent.com/ligueproleo/Foundation_Trilogy/main/Isaac_Asimov_Trilogia_Fundacao_UTF8.txt\" - Essa linha define a URL do arquivo de texto no GitHub que será utilizado para criar os embeddings.\n",
        "\n",
        "> response = requests.get(url) - Essa linha faz uma solicitação GET para a URL para obter o conteúdo do arquivo e armazena a resposta na variável response.\n",
        "\n",
        ">if response.status_code == 200 - Isso verifica o código de status da requisição e se a solicitação foi bem-sucedida, com base no código de status HTTP 200.\n",
        "\n",
        "> conteudo_do_arquivo = response.text - Esse comando verifica se a solicitação for bem-sucedida, e extrai o conteúdo do arquivo da resposta HTTP e o armazena na variável conteudo_do_arquivo.\n",
        "\n",
        "> print(conteudo_do_arquivo[:1000]) - Se a solicitação for bem-sucedida, esta linha imprime os primeiros 1000 caracteres do conteúdo do arquivo, o que é interessante para verificar se ele foi baixado corretamente, e ainda para entender o seu formato.\n",
        "\n",
        "> else:\n",
        "    print(f\"Erro ao acessar o arquivo. Código de status: {response.status_code}\")\n",
        "Aqui se faz o tratamento de erro da requisição. Se a solicitação falhar (código de status diferente de 200), essa parte do código imprime uma mensagem de erro com o código de status recebido.\n"
      ],
      "metadata": {
        "id": "BkJEHMKZaXip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Estruturar o Arquivo em Linhas\n",
        "#### Estruturar em linhas pode ser útil em tarefas de NLP - processamento de linguagem natural, análise de texto, ou qualquer aplicação que envolva a manipulação de dados de texto em formato de linha, como a contagem de palavras, análise de padrões, entre outras."
      ],
      "metadata": {
        "id": "i9s1jwuEejmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import re\n",
        "\n",
        "# URL do arquivo raw no GitHub\n",
        "url = \"https://raw.githubusercontent.com/ligueproleo/Foundation_Trilogy/main/Isaac_Asimov_Trilogia_Fundacao_UTF8.txt\"\n",
        "\n",
        "# read file into list of lines, ignoring errors\n",
        "with urllib.request.urlopen(url) as response:\n",
        "    lines = response.read().decode('utf-8', errors='ignore').split(\"\\n\")\n"
      ],
      "metadata": {
        "id": "fQMsCywwFjsG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Os comandos desse bloco realizam as seguintes tarefas\n",
        "\n",
        "> import urllib.request\n",
        "import re\n",
        "Isso importa as bibliotecas urllib.request para trabalhar com requisições HTTP e re para manipulação de expressões regulares.\n",
        "\n",
        ">url = \"https://raw.githubusercontent.com/ligueproleo/Foundation_Trilogy/main/Isaac_Asimov_Trilogia_Fundacao_UTF8.txt\"\n",
        "Essa linha define a URL do arquivo de texto no GitHub que será utilizado.\n",
        "\n",
        "> with urllib.request.urlopen(url) as response:\n",
        "    lines = response.read().decode('utf-8', errors='ignore').split(\"\\n\")\n",
        "\n",
        "Leirura do Arquivo em Linhas, Ignorando Erros:\n",
        "urllib.request.urlopen(url): Faz uma requisição para a URL especificada.\n",
        "response.read(): Lê o conteúdo da resposta.\n",
        ".decode('utf-8', errors='ignore'): Decodifica o conteúdo usando UTF-8, ignorando erros de codificação.\n",
        ".split(\"\\n\"): Divide o conteúdo em linhas, criando uma lista de strings, onde cada string representa uma linha do arquivo.\n"
      ],
      "metadata": {
        "id": "-XuixpdHfW8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pré-Processamento de Texto\n",
        "####  Importante em tarefas de processamento de linguagem natural (NLP), como a criação de modelos de word embeddings. Transforma o texto em um formato mais adequado para análise, removendo caracteres indesejados e dividindo-o em unidades menores (tokens). O resultado, a lista sentences, é uma lista de listas, onde cada lista interna contém os tokens de uma linha do arquivo original, pronta para ser utilizada em tarefas de modelagem de linguagem.  "
      ],
      "metadata": {
        "id": "LVVzwEX7gGbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "\n",
        "for line in lines:\n",
        "    # Converta a linha para minúsculas para ser case insensitive\n",
        "    line = line.lower()\n",
        "\n",
        "    # Remova pontuações e caracteres especiais, mantendo apenas letras e números\n",
        "    line = re.sub(r'[^a-zA-Z0-9\\s]', '', line)\n",
        "\n",
        "    # Divida a linha em tokens usando espaços em branco como delimitadores\n",
        "    tokens = line.split()\n",
        "\n",
        "    # Somente mantenha linhas com pelo menos um token\n",
        "    if len(tokens) > 0:\n",
        "        sentences.append(tokens)\n"
      ],
      "metadata": {
        "id": "R6ZlPoTJPp1z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Os comandos desse bloco realizam as seguintes tarefas\n",
        "\n",
        "\n",
        "> sentences = []: Cria uma lista vazia chamada sentences que será usada para armazenar as linhas processadas do arquivo\n",
        "\n",
        "> for line in lines: Percorre cada linha do arquivo, onde lines é uma lista de strings, representando as linhas do arquivo de texto.\n",
        "\n",
        "> line = line.lower(): Converte todas as letras da linha para minúsculas. Isso torna a tokenização case-insensitive, tratando palavras em maiúsculas e minúsculas da mesma forma.\n",
        "\n",
        ">line = re.sub(r'[^a-zA-Z0-9\\s]', '', line): Remove todas as pontuações e caracteres especiais, mantendo apenas letras e números. A expressão regular [^a-zA-Z0-9\\s] significa \"qualquer caractere que não seja uma letra maiúscula ou minúscula, número ou espaço em branco\". Isso ajuda a manter apenas os caracteres relevantes para a análise textual.\n",
        "\n",
        ">tokens = line.split(): Divide a linha em tokens usando espaços em branco como delimitadores. Isso cria uma lista de palavras da linha.\n",
        "\n",
        "> if len(tokens) > 0: e sentences.append(tokens): Garante que apenas linhas com pelo menos um token sejam adicionadas à lista sentences. Isso evita a inclusão de linhas em branco ou que consistem apenas de pontuações\n",
        "\n"
      ],
      "metadata": {
        "id": "JHJT7Ryxfv7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Treinar um Modelo Word2Vec\n",
        "####  Esse etapa é crucial para tarefas de processamento de linguagem natural, especificamente para criar representações vetoriais (embeddings) de palavras.\n",
        "#### O modelo Word2Vec treinado (bard2vec neste caso) pode ser usado para calcular similaridades semânticas entre palavras, encontrar palavras semanticamente relacionadas e, em geral, capturar a semântica distribucional do corpus de texto utilizado no treinamento. Esses embeddings de palavras são frequentemente utilizados em tarefas como classificação de texto, agrupamento, tradução automática, entre outras."
      ],
      "metadata": {
        "id": "QJaNuwMngOy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "bard2vec = Word2Vec(\n",
        "         sentences,\n",
        "         min_count=3,   # Ignora palavras que aparecem menos de 3 vezes\n",
        "         vector_size=50,       # Dimensionalidade dos embeddings de palavras\n",
        "         sg=1,        # Usa o método skip-gram (1 para skip-gram, 0 para CBOW)\n",
        "         window=7,      # Janela de contexto para palavras durante o treinamento\n",
        "         epochs=40)       # Número de épocas de treinamento sobre o corpus\n"
      ],
      "metadata": {
        "id": "3Rxg_9KjbhWm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Os comandos desse bloco usam a biblioteca gensin para treinar um modelo Word2Vec a partir das sentenças processadas anteriormente\n",
        "\n",
        "> from gensim.models import Word2Vec: Importa a classe Word2Vec da biblioteca Gensim, que é uma biblioteca popular para processamento de linguagem natural.\n",
        "\n",
        "> bard2vec = Word2Vec(: Cria uma instância do modelo Word2Vec chamada bard2vec.\n",
        "\n",
        ">sentences,: Passa as sentenças processadas como dados de entrada para o modelo.\n",
        "\n",
        ">min_count=3,: Ignora palavras que aparecem menos de 3 vezes no corpus. Isso ajuda a remover palavras muito raras que podem não contribuir significativamente para as representações vetoriais.\n",
        "\n",
        ">vector_size=50,: Define a dimensionalidade dos embeddings de palavras como 50. Isso determina o número de dimensões nos quais cada palavra será representada.\n",
        "\n",
        "> sg=1,: Usa o método skip-gram. O valor 1 indica que o método skip-gram será utilizado, enquanto o valor 0 indicaria o uso do método CBOW (Continuous Bag of Words).\n",
        "\n",
        "> window=7,: Define a janela de contexto para palavras durante o treinamento como 7. Isso significa que o modelo levará em consideração 7 palavras à esquerda e à direita de uma palavra alvo ao aprender as representações vetoriais.\n",
        "\n",
        "> epochs=40): Define o número de épocas de treinamento sobre o corpus como 40. Uma época é uma passagem completa pelos dados de treinamento.\n",
        "\n",
        "Podemos ainda, ajustar ajustar os parâmetros com base nas características específicas do corpus e das necessidades da aplicação. É uma questão de tentar alternativas.\n"
      ],
      "metadata": {
        "id": "8HU7CHgZfy_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Encontrar Similaridades\n",
        "\n",
        "#### Verificar as palavras mais similares à palavra 'mutante' com base nos embeddings de palavras (vetores) aprendidos pelo modelo Word2Vec (bard2vec)."
      ],
      "metadata": {
        "id": "ICeDHbTJpBVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bard2vec.wv.most_similar('mutante')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIcYXhVGnWME",
        "outputId": "c33b4f18-e68c-4042-cf02-bcbf9ebdb7ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('poderes', 0.6433465480804443),\n",
              " ('lidar', 0.6432638168334961),\n",
              " ('supe', 0.6381413340568542),\n",
              " ('conhecesse', 0.6337699294090271),\n",
              " ('imprevisvel', 0.6267795562744141),\n",
              " ('truque', 0.6229881644248962),\n",
              " ('humano', 0.6191799640655518),\n",
              " ('provinciano', 0.6158068180084229),\n",
              " ('desconhece', 0.6111360192298889),\n",
              " ('aconteceria', 0.6099813580513)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### bard2vec.wv.most_similar('mutante') : Procura por palavras semanticamente mais similares à palavra 'mutante' no modelo Word2Vec treinado (bard2vec).\n",
        "\n",
        "\n",
        "> bard2vec: Este é o modelo Word2Vec treinado que contém as representações vetoriais de palavras.\n",
        "\n",
        "> wv: Este é o atributo do modelo Word2Vec que representa o Word Vectors (vetores de palavras).\n",
        "\n",
        "> most_similar('mutante'): Este método encontra as palavras mais similares à palavra fornecida como argumento ('mutante' neste caso) com base nas representações vetoriais aprendidas durante o treinamento do modelo. O resultado é uma lista de tuplas onde cada tupla contém uma palavra similar e uma pontuação de similaridade.\n",
        "\n",
        "Essas palavras similares são determinadas pela proximidade de vetores no espaço vetorial de palavras, refletindo a coocorrência e similaridade contextual durante o treinamento do modelo."
      ],
      "metadata": {
        "id": "EjEC4Dfrkp2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A seguir criamos uma alternativa para treinar um modelo, mas não achamos interessante porque o fasttext exigia que o arquivo de dados fosse baixado localmente, e nós já havíamos criado um repositório no github e queríamos usa-lo diretamente. Todavia, ele está aí, e funcionando para comparação."
      ],
      "metadata": {
        "id": "kGPtwvOnat3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Treinar Modelo Alternativo FastText - Skip-gram Word2Vec\n",
        "####  Esse bloco alternativo no exercício, instala a biblioteca FastText e a utilizam para treinar um modelo Skip-gram Word2Vec de 50 dimensões com parâmetros específicos e, por fim, salva o modelo treinado em um arquivo binário."
      ],
      "metadata": {
        "id": "wwzPrKqtgaBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "import urllib.request\n",
        "\n",
        "# Baixa o arquivo de texto localmente\n",
        "url = \"https://raw.githubusercontent.com/ligueproleo/Foundation_Trilogy/main/Isaac_Asimov_Trilogia_Fundacao_UTF8.txt\"\n",
        "local_filename, headers = urllib.request.urlretrieve(url, filename=\"local_text_file.txt\")\n",
        "\n",
        "# Cria um modelo Word2Vec usando fasttext\n",
        "model = fasttext.train_unsupervised(input=local_filename, model='skipgram', dim=50, epoch=40, minCount=3, ws=7)\n",
        "\n",
        "# Salva o modelo treinado\n",
        "model.save_model('fasttext_skipgram_model.bin')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6vvfW9bUek7",
        "outputId": "600433ad-9367-4495-f159-7aefe7d38a62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=6308f1d40b9f73e7543f7d04fd80045e81c000f4b67eecda0414ba14ceaa5267\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Os comandos desse bloco realizam as seguintes tarefas\n",
        "\n",
        "> !pip install fasttext: Instala a biblioteca FastText.\n",
        "\n",
        "> url = \"https://raw.githubusercontent.com/ligueproleo/Foundation_Trilogy/main/Isaac_Asimov_Trilogia_Fundacao_UTF8.txt\"\n",
        "local_filename, headers = urllib.request.urlretrieve(url, filename=\"local_text_file.txt\"): Baixa o arquivo de texto da URL especificada e o salva localmente com o nome \"local_text_file.txt\".\n",
        "\n",
        "> model = fasttext.train_unsupervised(input=local_filename, model='skipgram', dim=50, epoch=40, minCount=3, ws=7)\n",
        "Cria um modelo Word2Vec Skip-gram de 50 dimensões utilizando o FastText, treinando-o no arquivo de texto local com parâmetros específicos, como 40 épocas de treinamento, ignorando palavras que aparecem menos de 3 vezes, e uma janela de contexto de 7 palavras.\n",
        "\n",
        "> model.save_model('fasttext_skipgram_model.bin')\n",
        "Salva o modelo treinado em um arquivo binário chamado 'fasttext_skipgram_model.bin'.\n",
        "\n"
      ],
      "metadata": {
        "id": "5CmsVxVycF9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Encontrar Similaridades no Modelo Alternativo\n"
      ],
      "metadata": {
        "id": "7JuWFUPTgdPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o modelo treinado\n",
        "model = fasttext.load_model('fasttext_skipgram_model.bin')\n",
        "\n",
        "# Encontra palavras mais similares\n",
        "similar_words = model.get_nearest_neighbors('mutante', k=7)\n",
        "\n",
        "# Imprime palavras mais similares à palavra 'mutante'\n",
        "print(f'Palavras mais similares à \"mutante\":')\n",
        "for word, score in similar_words:\n",
        "    print(f'{word}: {score}')\n"
      ],
      "metadata": {
        "id": "CM4Y8t3ZmYLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2a1a75-04a4-4ae5-f73a-cdb04e20fa1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palavras mais similares à \"mutante\":\n",
            "0.8703683018684387: mutante!\n",
            "0.850628674030304: mutante,\n",
            "0.8039129376411438: mutante.\n",
            "0.753200113773346: mutação\n",
            "0.6755052208900452: relutante\n",
            "0.6698078513145447: desenvolvido\n",
            "0.6614865064620972: razoável\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    }
  ]
}